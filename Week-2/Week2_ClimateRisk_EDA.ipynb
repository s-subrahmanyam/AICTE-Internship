{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8a2d0c",
   "metadata": {},
   "source": [
    "\n",
    "# Week 2 — Climate Risk & Disaster Management  \n",
    "**Dataset:** World Disaster Risk Index Time Series  \n",
    "**Goal:** *Predict the next year's World Risk Index (WRI) for a chosen country.*\n",
    "\n",
    "This notebook continues from Week 1 and adds:\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Data Cleaning & Transformations\n",
    "- Feature Engineering (including next-year target)\n",
    "- Feature Selection (Correlation filter, SelectKBest, RandomForest importance)\n",
    "\n",
    "> **Note:** Place the dataset CSV in the same folder as this notebook and set `DATA_FILE` accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b646722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ML / feature selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Display options\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "DATA_FILE = \"global_disaster_risk_index_time_series.csv\"  # <-- change if your filename is different\n",
    "assert Path(DATA_FILE).exists(), f\"Dataset file '{DATA_FILE}' not found. Place it next to this notebook.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Load & basic cleaning\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "# Strip whitespace from column names to avoid accidental mismatches\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39662b4",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4091cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Info and basic stats\n",
    "print(\"----- INFO -----\")\n",
    "df.info()\n",
    "print(\"\\n----- DESCRIBE (numeric) -----\")\n",
    "display(df.describe())\n",
    "print(\"\\n----- Missing Values -----\")\n",
    "display(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca365929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Distributions for key numeric features\n",
    "numeric_cols = ['WRI', 'Exposure', 'Vulnerability', 'Susceptibility', \n",
    "                'Lack of Coping Capabilities', 'Lack of Adaptive Capacities', 'Year']\n",
    "numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure()\n",
    "    plt.hist(df[col].dropna(), bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Average WRI trend over years\n",
    "if 'Year' in df.columns and 'WRI' in df.columns:\n",
    "    wri_year = df.groupby('Year')['WRI'].mean()\n",
    "    plt.figure()\n",
    "    plt.plot(wri_year.index, wri_year.values, marker='o')\n",
    "    plt.title(\"Average WRI Over Years\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Average WRI\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top 10 regions by average WRI\n",
    "if 'Region' in df.columns and 'WRI' in df.columns:\n",
    "    top_regions = df.groupby('Region')['WRI'].mean().sort_values(ascending=False).head(10)\n",
    "    display(top_regions.to_frame('Avg WRI'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f6b20e",
   "metadata": {},
   "source": [
    "## Feature Engineering — Next-Year Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36fcef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create target: next year's WRI per Region (shift -1)\n",
    "# This means: for each Region and Year, we want to predict the WRI for Year+1\n",
    "df = df.sort_values(['Region', 'Year'])\n",
    "\n",
    "if 'Region' in df.columns and 'Year' in df.columns and 'WRI' in df.columns:\n",
    "    df['WRI_next'] = df.groupby('Region')['WRI'].shift(-1)\n",
    "else:\n",
    "    raise ValueError(\"Required columns 'Region', 'Year', 'WRI' are missing.\")\n",
    "\n",
    "print(\"Rows before dropping NA target:\", len(df))\n",
    "df_model = df.dropna(subset=['WRI_next']).copy()\n",
    "print(\"Rows after dropping NA target:\", len(df_model))\n",
    "\n",
    "display(df_model.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0e07f",
   "metadata": {},
   "source": [
    "## Data Preparation — Imputation, Encoding, Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff84442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify features\n",
    "cat_cols = [c for c in ['Region', 'Exposure Category', 'WRI Category',\n",
    "                        'Vulnerability Category', 'Susceptibility Category'] if c in df_model.columns]\n",
    "\n",
    "num_cols = [c for c in ['WRI', 'Exposure', 'Vulnerability', 'Susceptibility',\n",
    "                        'Lack of Coping Capabilities', 'Lack of Adaptive Capacities', 'Year']\n",
    "            if c in df_model.columns]\n",
    "\n",
    "target = 'WRI_next'\n",
    "features = cat_cols + num_cols\n",
    "\n",
    "print(\"Categorical:\", cat_cols)\n",
    "print(\"Numeric:\", num_cols)\n",
    "\n",
    "X = df_model[features].copy()\n",
    "y = df_model[target].copy()\n",
    "\n",
    "# Column transformer: impute & encode categoricals, impute & scale numerics\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Fit transform to create processed feature matrix\n",
    "X_processed = preprocess.fit_transform(X)\n",
    "print(\"Processed feature matrix shape:\", X_processed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477f5cc",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d523b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Correlation filter (on numeric-only columns)\n",
    "corr_info = {}\n",
    "if num_cols:\n",
    "    corr_df = pd.concat([X[num_cols], y], axis=1).dropna()\n",
    "    corr = corr_df.corr(numeric_only=True)\n",
    "    display(corr[['WRI_next']].sort_values(by='WRI_next', ascending=False))\n",
    "    corr_info = corr[['WRI_next']].to_dict()['WRI_next']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) SelectKBest with f_regression on processed features\n",
    "# Note: We don't have original column names after OneHot; this is a ranking demo.\n",
    "k = min(20, X_processed.shape[1])  # pick top 20 or less\n",
    "skb = SelectKBest(score_func=f_regression, k=k)\n",
    "X_skb = skb.fit_transform(X_processed, y)\n",
    "\n",
    "print(\"Top-k features selected (indices):\", np.where(skb.get_support())[0].tolist())\n",
    "print(\"Corresponding F-scores (first 10):\", np.sort(skb.scores_[skb.get_support()])[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) RandomForest feature importances (rough guidance)\n",
    "rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_processed, y)\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Show top 20 importance indices and values\n",
    "top_idx = np.argsort(importances)[::-1][:20]\n",
    "print(\"Top 20 feature importance indices:\", top_idx.tolist())\n",
    "print(\"Top 20 importances:\", importances[top_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87af15",
   "metadata": {},
   "source": [
    "## Save Cleaned & Modeled-Ready Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save a cleaned version for Week 3 (optional)\n",
    "clean_out = \"clean_wdris_for_model.csv\"\n",
    "df_model.to_csv(clean_out, index=False)\n",
    "print(\"Saved:\", clean_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f16d8",
   "metadata": {},
   "source": [
    "\n",
    "## Improvisations (Highlights to paste in LMS)\n",
    "- Created **next-year target** `WRI_next` per Region using group-wise shift.\n",
    "- Performed **EDA** (info, describe, missing values, numeric distributions, trend by year).\n",
    "- Cleaned column names and **imputed** missing values (mean for numeric, mode for categorical).\n",
    "- Applied **One-Hot Encoding** for categories and **Standard Scaling** for numeric features.\n",
    "- Ran **three feature selection approaches**: numeric correlation with target, SelectKBest, and RandomForest importances.\n",
    "- Exported a **clean, model-ready CSV** for downstream modeling.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
